{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec40bb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mirisxinran\u001b[0m (\u001b[33mntuwb\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import wandb\n",
    "wandb.login()\n",
    "import random\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import GRU, Linear, ReLU, Sequential\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import NNConv, Set2Set, MessagePassing, global_mean_pool, aggr,GCNConv\n",
    "from torch_geometric.utils import remove_self_loops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df292bf5",
   "metadata": {},
   "source": [
    "Each model and target combination was trained using a uniform random hyper parameter search with 50 trials. \n",
    "\n",
    "T was costrained to be in the range 3 ≤ T ≤ 8 (in practice, any T ≥ 3 works). \n",
    "\n",
    "The number of set2set computations M was chosen from the range 1 ≤ M ≤ 12. \n",
    "\n",
    "All models were trained using SGD with the ADAM optimizer (Kingma &Ba (2014)), with batch size 20 for 3 million steps ( 540 epochs). \n",
    "\n",
    "The initial learning rate was chosen uniformly between 1e−5 and 5e−4. We used a linear learning rate decay that began between 10% and 90% of the way through training and the initial learning rate l decayed to a final learning ate l ∗ F, using a decay factor F in the range [.01, 1].\n",
    "\n",
    "The QM-9 dataset has 130462 molecules in it. We randomly chose 10000 samples for validation, 10000 samples or testing, and used the rest for training. We use the validation set to do early stopping and model selection and we eport scores on the test set. \n",
    "\n",
    "All targets were normalized o have mean 0 and variance 1. We minimize the mean quared error between the model output and the target, although we evaluate mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84f4827",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af0cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: l48o87j4\n",
      "Sweep URL: https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xnfcgzeh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 96\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.004764430672401777\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: mean\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_134549-xnfcgzeh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/xnfcgzeh\" target=\"_blank\">iconic-sweep-1</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/xnfcgzeh\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/xnfcgzeh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=mean, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.2639\n",
      "Epoch: 0, Validation Loss: 1.2144\n",
      "Epoch: 1, Training Loss: 1.1796\n",
      "Epoch: 1, Validation Loss: 1.1411\n",
      "Epoch: 2, Training Loss: 1.0920\n",
      "Epoch: 2, Validation Loss: 1.0450\n",
      "Epoch: 3, Training Loss: 0.9929\n",
      "Epoch: 3, Validation Loss: 0.9436\n",
      "Epoch: 4, Training Loss: 0.9525\n",
      "Epoch: 4, Validation Loss: 0.9516\n",
      "Epoch: 5, Training Loss: 0.9276\n",
      "Epoch: 5, Validation Loss: 0.8934\n",
      "Epoch: 6, Training Loss: 0.9096\n",
      "Epoch: 6, Validation Loss: 0.9139\n",
      "Epoch: 7, Training Loss: 0.8995\n",
      "Epoch: 7, Validation Loss: 0.9136\n",
      "Epoch: 8, Training Loss: 0.8912\n",
      "Epoch: 8, Validation Loss: 0.8535\n",
      "Epoch: 9, Training Loss: 0.8871\n",
      "Epoch: 9, Validation Loss: 0.8655\n",
      "Epoch: 10, Training Loss: 0.8769\n",
      "Epoch: 10, Validation Loss: 0.8901\n",
      "Epoch: 11, Training Loss: 0.8652\n",
      "Epoch: 11, Validation Loss: 0.9784\n",
      "Epoch: 12, Training Loss: 0.8652\n",
      "Epoch: 12, Validation Loss: 0.8634\n",
      "Epoch: 13, Training Loss: 0.8589\n",
      "Epoch: 13, Validation Loss: 0.8667\n",
      "Epoch: 14, Training Loss: 0.8558\n",
      "Epoch: 14, Validation Loss: 0.8519\n",
      "Epoch: 15, Training Loss: 0.8509\n",
      "Epoch: 15, Validation Loss: 0.8670\n",
      "Epoch: 16, Training Loss: 0.8467\n",
      "Epoch: 16, Validation Loss: 0.8195\n",
      "Epoch: 17, Training Loss: 0.8416\n",
      "Epoch: 17, Validation Loss: 0.8598\n",
      "Epoch: 18, Training Loss: 0.8375\n",
      "Epoch: 18, Validation Loss: 0.8319\n",
      "Epoch: 19, Training Loss: 0.8346\n",
      "Epoch: 19, Validation Loss: 0.8645\n",
      "Epoch: 20, Training Loss: 0.8356\n",
      "Epoch: 20, Validation Loss: 0.8095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Training Loss: 0.8337\n",
      "Epoch: 21, Validation Loss: 0.8368\n",
      "Epoch: 22, Training Loss: 0.8269\n",
      "Epoch: 22, Validation Loss: 0.8311\n",
      "Epoch: 23, Training Loss: 0.8301\n",
      "Epoch: 23, Validation Loss: 0.8551\n",
      "Epoch: 24, Training Loss: 0.8230\n",
      "Epoch: 24, Validation Loss: 0.8960\n",
      "Epoch: 25, Training Loss: 0.8187\n",
      "Epoch: 25, Validation Loss: 0.8164\n",
      "Epoch: 26, Training Loss: 0.8184\n",
      "Epoch: 26, Validation Loss: 0.8262\n",
      "Epoch: 27, Training Loss: 0.8178\n",
      "Epoch: 27, Validation Loss: 0.8272\n",
      "Epoch: 28, Training Loss: 0.8162\n",
      "Epoch: 28, Validation Loss: 0.8059\n",
      "Epoch: 29, Training Loss: 0.8163\n",
      "Epoch: 29, Validation Loss: 0.8278\n",
      "Epoch: 30, Training Loss: 0.8131\n",
      "Epoch: 30, Validation Loss: 0.8205\n",
      "Epoch: 31, Training Loss: 0.8099\n",
      "Epoch: 31, Validation Loss: 0.8294\n",
      "Epoch: 32, Training Loss: 0.8083\n",
      "Epoch: 32, Validation Loss: 0.8189\n",
      "Epoch: 33, Training Loss: 0.8085\n",
      "Epoch: 33, Validation Loss: 0.8191\n",
      "Epoch: 34, Training Loss: 0.8053\n",
      "Epoch: 34, Validation Loss: 0.8238\n",
      "Epoch: 35, Training Loss: 0.8026\n",
      "Epoch: 35, Validation Loss: 0.7962\n",
      "Epoch: 36, Training Loss: 0.8017\n",
      "Epoch: 36, Validation Loss: 0.8056\n",
      "Epoch: 37, Training Loss: 0.8010\n",
      "Epoch: 37, Validation Loss: 0.8104\n",
      "Epoch: 38, Training Loss: 0.7981\n",
      "Epoch: 38, Validation Loss: 0.7886\n",
      "Epoch: 39, Training Loss: 0.7941\n",
      "Epoch: 39, Validation Loss: 0.7903\n",
      "Epoch: 40, Training Loss: 0.7955\n",
      "Epoch: 40, Validation Loss: 0.7936\n",
      "Epoch: 41, Training Loss: 0.7955\n",
      "Epoch: 41, Validation Loss: 0.8005\n",
      "Epoch: 42, Training Loss: 0.7939\n",
      "Epoch: 42, Validation Loss: 0.7877\n",
      "Epoch: 43, Training Loss: 0.7892\n",
      "Epoch: 43, Validation Loss: 0.7911\n",
      "Epoch: 44, Training Loss: 0.7900\n",
      "Epoch: 44, Validation Loss: 0.7948\n",
      "Epoch: 45, Training Loss: 0.7893\n",
      "Epoch: 45, Validation Loss: 0.7697\n",
      "Epoch: 46, Training Loss: 0.7875\n",
      "Epoch: 46, Validation Loss: 0.8088\n",
      "Epoch: 47, Training Loss: 0.7911\n",
      "Epoch: 47, Validation Loss: 0.8469\n",
      "Epoch: 48, Training Loss: 0.7866\n",
      "Epoch: 48, Validation Loss: 0.7960\n",
      "Epoch: 49, Training Loss: 0.7864\n",
      "Epoch: 49, Validation Loss: 0.7871\n",
      "Epoch: 50, Training Loss: 0.7836\n",
      "Epoch: 50, Validation Loss: 0.8037\n",
      "Epoch: 51, Training Loss: 0.7876\n",
      "Epoch: 51, Validation Loss: 0.7813\n",
      "Epoch: 52, Training Loss: 0.7830\n",
      "Epoch: 52, Validation Loss: 0.7888\n",
      "Epoch: 53, Training Loss: 0.7816\n",
      "Epoch: 53, Validation Loss: 0.7983\n",
      "Epoch: 54, Training Loss: 0.7801\n",
      "Epoch: 54, Validation Loss: 0.8268\n",
      "Epoch: 55, Training Loss: 0.7788\n",
      "Epoch: 55, Validation Loss: 0.7660\n",
      "Epoch: 56, Training Loss: 0.7775\n",
      "Epoch: 56, Validation Loss: 0.7613\n",
      "Epoch: 57, Training Loss: 0.7743\n",
      "Epoch: 57, Validation Loss: 0.7572\n",
      "Epoch: 58, Training Loss: 0.7750\n",
      "Epoch: 58, Validation Loss: 0.7728\n",
      "Epoch: 59, Training Loss: 0.7760\n",
      "Epoch: 59, Validation Loss: 0.7644\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▇▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▄▄▃▃▃▃▃▃▃▂▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>59</td></tr><tr><td>train_loss</td><td>0.776</td></tr><tr><td>val_loss</td><td>0.76438</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">iconic-sweep-1</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/xnfcgzeh\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/xnfcgzeh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_134549-xnfcgzeh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m5xh8wsl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.04568275256685655\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: mean\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_151334-m5xh8wsl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/m5xh8wsl\" target=\"_blank\">classic-sweep-2</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/m5xh8wsl\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/m5xh8wsl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=mean, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.6660\n",
      "Epoch: 0, Validation Loss: 1.5029\n",
      "Epoch: 1, Training Loss: 1.4879\n",
      "Epoch: 1, Validation Loss: 1.4781\n",
      "Epoch: 2, Training Loss: 1.4905\n",
      "Epoch: 2, Validation Loss: 1.5932\n",
      "Epoch: 3, Training Loss: 1.5586\n",
      "Epoch: 3, Validation Loss: 1.5439\n",
      "Epoch: 4, Training Loss: 1.5663\n",
      "Epoch: 4, Validation Loss: 1.5438\n",
      "Epoch: 5, Training Loss: 1.5309\n",
      "Epoch: 5, Validation Loss: 1.5516\n",
      "Epoch: 6, Training Loss: 1.5320\n",
      "Epoch: 6, Validation Loss: 1.5407\n",
      "Epoch: 7, Training Loss: 1.5308\n",
      "Epoch: 7, Validation Loss: 1.5455\n",
      "Epoch: 8, Training Loss: 1.5314\n",
      "Epoch: 8, Validation Loss: 1.5499\n",
      "Epoch: 9, Training Loss: 1.5330\n",
      "Epoch: 9, Validation Loss: 1.5424\n",
      "Epoch: 10, Training Loss: 1.5311\n",
      "Epoch: 10, Validation Loss: 1.5897\n",
      "Epoch: 11, Training Loss: 1.5322\n",
      "Epoch: 11, Validation Loss: 1.5381\n",
      "Epoch: 12, Training Loss: 1.5308\n",
      "Epoch: 12, Validation Loss: 1.5556\n",
      "Epoch: 13, Training Loss: 1.5310\n",
      "Epoch: 13, Validation Loss: 1.5343\n",
      "Epoch: 14, Training Loss: 1.5315\n",
      "Epoch: 14, Validation Loss: 1.5464\n",
      "Epoch: 15, Training Loss: 1.5300\n",
      "Epoch: 15, Validation Loss: 1.5346\n",
      "Epoch: 16, Training Loss: 1.5317\n",
      "Epoch: 16, Validation Loss: 1.5349\n",
      "Epoch: 17, Training Loss: 1.5313\n",
      "Epoch: 17, Validation Loss: 1.5368\n",
      "Epoch: 18, Training Loss: 1.5312\n",
      "Epoch: 18, Validation Loss: 1.5509\n",
      "Epoch: 19, Training Loss: 1.5316\n",
      "Epoch: 19, Validation Loss: 1.5382\n",
      "Epoch: 20, Training Loss: 1.5311\n",
      "Epoch: 20, Validation Loss: 1.5559\n",
      "Epoch: 21, Training Loss: 1.5304\n",
      "Epoch: 21, Validation Loss: 1.5420\n",
      "Epoch: 22, Training Loss: 1.5299\n",
      "Epoch: 22, Validation Loss: 1.5432\n",
      "Epoch: 23, Training Loss: 1.5310\n",
      "Epoch: 23, Validation Loss: 1.5372\n",
      "Epoch: 24, Training Loss: 1.5317\n",
      "Epoch: 24, Validation Loss: 1.5553\n",
      "Epoch: 25, Training Loss: 1.5322\n",
      "Epoch: 25, Validation Loss: 1.5480\n",
      "Epoch: 26, Training Loss: 1.5308\n",
      "Epoch: 26, Validation Loss: 1.5515\n",
      "Epoch: 27, Training Loss: 1.5317\n",
      "Epoch: 27, Validation Loss: 1.5346\n",
      "Epoch: 28, Training Loss: 1.5314\n",
      "Epoch: 28, Validation Loss: 1.5374\n",
      "Epoch: 29, Training Loss: 1.5328\n",
      "Epoch: 29, Validation Loss: 1.5778\n",
      "Epoch: 30, Training Loss: 1.5327\n",
      "Epoch: 30, Validation Loss: 1.5483\n",
      "Epoch: 31, Training Loss: 1.5310\n",
      "Epoch: 31, Validation Loss: 1.6010\n",
      "Epoch: 32, Training Loss: 1.5306\n",
      "Epoch: 32, Validation Loss: 1.5387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Training Loss: 1.5318\n",
      "Epoch: 33, Validation Loss: 1.5451\n",
      "Epoch: 34, Training Loss: 1.5328\n",
      "Epoch: 34, Validation Loss: 1.5419\n",
      "Epoch: 35, Training Loss: 1.5316\n",
      "Epoch: 35, Validation Loss: 1.5420\n",
      "Epoch: 36, Training Loss: 1.5299\n",
      "Epoch: 36, Validation Loss: 1.5644\n",
      "Epoch: 37, Training Loss: 1.5315\n",
      "Epoch: 37, Validation Loss: 1.5421\n",
      "Epoch: 38, Training Loss: 1.5323\n",
      "Epoch: 38, Validation Loss: 1.5443\n",
      "Epoch: 39, Training Loss: 1.5310\n",
      "Epoch: 39, Validation Loss: 1.5631\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▁▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>val_loss</td><td>▂▁█▅▅▅▅▅▅▅▇▄▅▄▅▄▄▄▅▄▅▅▅▄▅▅▅▄▄▇▅█▄▅▅▅▆▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>1.53099</td></tr><tr><td>val_loss</td><td>1.56314</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classic-sweep-2</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/m5xh8wsl\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/m5xh8wsl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_151334-m5xh8wsl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q0jz0zha with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 232\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.02785491249718454\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: add\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_163508-q0jz0zha</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/q0jz0zha\" target=\"_blank\">lucky-sweep-3</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/q0jz0zha\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/q0jz0zha</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=add, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 5.6211\n",
      "Epoch: 0, Validation Loss: 1.2860\n",
      "Epoch: 1, Training Loss: 1.2684\n",
      "Epoch: 1, Validation Loss: 1.2394\n",
      "Epoch: 2, Training Loss: 1.2417\n",
      "Epoch: 2, Validation Loss: 1.2846\n",
      "Epoch: 3, Training Loss: 1.2309\n",
      "Epoch: 3, Validation Loss: 1.2407\n",
      "Epoch: 4, Training Loss: 1.2315\n",
      "Epoch: 4, Validation Loss: 1.2221\n",
      "Epoch: 5, Training Loss: 1.2331\n",
      "Epoch: 5, Validation Loss: 1.3164\n",
      "Epoch: 6, Training Loss: 1.2314\n",
      "Epoch: 6, Validation Loss: 1.2104\n",
      "Epoch: 7, Training Loss: 1.2282\n",
      "Epoch: 7, Validation Loss: 1.2185\n",
      "Epoch: 8, Training Loss: 1.2227\n",
      "Epoch: 8, Validation Loss: 1.2435\n",
      "Epoch: 9, Training Loss: 1.2285\n",
      "Epoch: 9, Validation Loss: 1.2953\n",
      "Epoch: 10, Training Loss: 1.2294\n",
      "Epoch: 10, Validation Loss: 1.2607\n",
      "Epoch: 11, Training Loss: 1.2247\n",
      "Epoch: 11, Validation Loss: 1.2303\n",
      "Epoch: 12, Training Loss: 1.2240\n",
      "Epoch: 12, Validation Loss: 1.2035\n",
      "Epoch: 13, Training Loss: 1.2188\n",
      "Epoch: 13, Validation Loss: 1.1990\n",
      "Epoch: 14, Training Loss: 1.2116\n",
      "Epoch: 14, Validation Loss: 1.2281\n",
      "Epoch: 15, Training Loss: 1.2068\n",
      "Epoch: 15, Validation Loss: 1.2184\n",
      "Epoch: 16, Training Loss: 1.2000\n",
      "Epoch: 16, Validation Loss: 1.1812\n",
      "Epoch: 17, Training Loss: 1.1950\n",
      "Epoch: 17, Validation Loss: 1.1806\n",
      "Epoch: 18, Training Loss: 1.1983\n",
      "Epoch: 18, Validation Loss: 1.2035\n",
      "Epoch: 19, Training Loss: 1.2172\n",
      "Epoch: 19, Validation Loss: 1.1674\n",
      "Epoch: 20, Training Loss: 1.1814\n",
      "Epoch: 20, Validation Loss: 1.1565\n",
      "Epoch: 21, Training Loss: 1.1876\n",
      "Epoch: 21, Validation Loss: 1.2156\n",
      "Epoch: 22, Training Loss: 1.2197\n",
      "Epoch: 22, Validation Loss: 1.2496\n",
      "Epoch: 23, Training Loss: 1.2103\n",
      "Epoch: 23, Validation Loss: 1.1795\n",
      "Epoch: 24, Training Loss: 1.1911\n",
      "Epoch: 24, Validation Loss: 1.2500\n",
      "Epoch: 25, Training Loss: 1.2134\n",
      "Epoch: 25, Validation Loss: 1.2406\n",
      "Epoch: 26, Training Loss: 1.2076\n",
      "Epoch: 26, Validation Loss: 1.2070\n",
      "Epoch: 27, Training Loss: 1.2195\n",
      "Epoch: 27, Validation Loss: 1.2167\n",
      "Epoch: 28, Training Loss: 1.2116\n",
      "Epoch: 28, Validation Loss: 1.2557\n",
      "Epoch: 29, Training Loss: 1.2092\n",
      "Epoch: 29, Validation Loss: 1.2531\n",
      "Epoch: 30, Training Loss: 1.2133\n",
      "Epoch: 30, Validation Loss: 1.2181\n",
      "Epoch: 31, Training Loss: 1.2145\n",
      "Epoch: 31, Validation Loss: 1.2412\n",
      "Epoch: 32, Training Loss: 1.2134\n",
      "Epoch: 32, Validation Loss: 1.2356\n",
      "Epoch: 33, Training Loss: 1.2133\n",
      "Epoch: 33, Validation Loss: 1.2187\n",
      "Epoch: 34, Training Loss: 1.2155\n",
      "Epoch: 34, Validation Loss: 1.2323\n",
      "Epoch: 35, Training Loss: 1.2149\n",
      "Epoch: 35, Validation Loss: 1.2414\n",
      "Epoch: 36, Training Loss: 1.2200\n",
      "Epoch: 36, Validation Loss: 1.2201\n",
      "Epoch: 37, Training Loss: 1.2155\n",
      "Epoch: 37, Validation Loss: 1.2664\n",
      "Epoch: 38, Training Loss: 1.2128\n",
      "Epoch: 38, Validation Loss: 1.2135\n",
      "Epoch: 39, Training Loss: 1.2150\n",
      "Epoch: 39, Validation Loss: 1.2282\n",
      "Epoch: 40, Training Loss: 1.2177\n",
      "Epoch: 40, Validation Loss: 1.2538\n",
      "Epoch: 41, Training Loss: 1.2125\n",
      "Epoch: 41, Validation Loss: 1.2233\n",
      "Epoch: 42, Training Loss: 1.2185\n",
      "Epoch: 42, Validation Loss: 1.2299\n",
      "Epoch: 43, Training Loss: 1.2208\n",
      "Epoch: 43, Validation Loss: 1.2200\n",
      "Epoch: 44, Training Loss: 1.2108\n",
      "Epoch: 44, Validation Loss: 1.2217\n",
      "Epoch: 45, Training Loss: 1.2138\n",
      "Epoch: 45, Validation Loss: 1.2559\n",
      "Epoch: 46, Training Loss: 1.2188\n",
      "Epoch: 46, Validation Loss: 1.2209\n",
      "Epoch: 47, Training Loss: 1.2146\n",
      "Epoch: 47, Validation Loss: 1.2207\n",
      "Epoch: 48, Training Loss: 1.2175\n",
      "Epoch: 48, Validation Loss: 1.2290\n",
      "Epoch: 49, Training Loss: 1.2190\n",
      "Epoch: 49, Validation Loss: 1.2219\n",
      "Epoch: 50, Training Loss: 1.2188\n",
      "Epoch: 50, Validation Loss: 1.2361\n",
      "Epoch: 51, Training Loss: 1.2167\n",
      "Epoch: 51, Validation Loss: 1.2350\n",
      "Epoch: 52, Training Loss: 1.2137\n",
      "Epoch: 52, Validation Loss: 1.2262\n",
      "Epoch: 53, Training Loss: 1.2133\n",
      "Epoch: 53, Validation Loss: 1.2213\n",
      "Epoch: 54, Training Loss: 1.2193\n",
      "Epoch: 54, Validation Loss: 1.2364\n",
      "Epoch: 55, Training Loss: 1.2148\n",
      "Epoch: 55, Validation Loss: 1.2322\n",
      "Epoch: 56, Training Loss: 1.2196\n",
      "Epoch: 56, Validation Loss: 1.2348\n",
      "Epoch: 57, Training Loss: 1.2168\n",
      "Epoch: 57, Validation Loss: 1.2778\n",
      "Epoch: 58, Training Loss: 1.2142\n",
      "Epoch: 58, Validation Loss: 1.2207\n",
      "Epoch: 59, Training Loss: 1.2202\n",
      "Epoch: 59, Validation Loss: 1.2185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13259fc514074acdb13891b9996ce6dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▇▅▅▄▃▄█▆▃▃▄▂▃▁▄▅▆▅▄▆▄▅▄▅▄▆▄▆▄▄▆▄▄▄▅▄▅▅▇▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>59</td></tr><tr><td>train_loss</td><td>1.22022</td></tr><tr><td>val_loss</td><td>1.21846</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-sweep-3</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/q0jz0zha\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/q0jz0zha</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_163508-q0jz0zha/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fw2sbetj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.05708627000307257\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: add\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_171703-fw2sbetj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/fw2sbetj\" target=\"_blank\">vibrant-sweep-4</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/fw2sbetj\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/fw2sbetj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=add, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 11.6413\n",
      "Epoch: 0, Validation Loss: 1.3365\n",
      "Epoch: 1, Training Loss: 1.3694\n",
      "Epoch: 1, Validation Loss: 1.3812\n",
      "Epoch: 2, Training Loss: 1.3724\n",
      "Epoch: 2, Validation Loss: 1.2952\n",
      "Epoch: 3, Training Loss: 1.3679\n",
      "Epoch: 3, Validation Loss: 1.4378\n",
      "Epoch: 4, Training Loss: 1.3724\n",
      "Epoch: 4, Validation Loss: 1.3767\n",
      "Epoch: 5, Training Loss: 1.3616\n",
      "Epoch: 5, Validation Loss: 1.3203\n",
      "Epoch: 6, Training Loss: 1.3673\n",
      "Epoch: 6, Validation Loss: 1.3371\n",
      "Epoch: 7, Training Loss: 1.3656\n",
      "Epoch: 7, Validation Loss: 1.4934\n",
      "Epoch: 8, Training Loss: 1.3720\n",
      "Epoch: 8, Validation Loss: 1.3011\n",
      "Epoch: 9, Training Loss: 1.3738\n",
      "Epoch: 9, Validation Loss: 1.2885\n",
      "Epoch: 10, Training Loss: 1.3584\n",
      "Epoch: 10, Validation Loss: 1.2805\n",
      "Epoch: 11, Training Loss: 1.3620\n",
      "Epoch: 11, Validation Loss: 1.3114\n",
      "Epoch: 12, Training Loss: 1.3562\n",
      "Epoch: 12, Validation Loss: 1.4181\n",
      "Epoch: 13, Training Loss: 1.3537\n",
      "Epoch: 13, Validation Loss: 1.2861\n",
      "Epoch: 14, Training Loss: 1.3485\n",
      "Epoch: 14, Validation Loss: 1.3169\n",
      "Epoch: 15, Training Loss: 1.3391\n",
      "Epoch: 15, Validation Loss: 1.2456\n",
      "Epoch: 16, Training Loss: 1.3493\n",
      "Epoch: 16, Validation Loss: 1.2120\n",
      "Epoch: 17, Training Loss: 1.3459\n",
      "Epoch: 17, Validation Loss: 1.2591\n",
      "Epoch: 18, Training Loss: 1.3282\n",
      "Epoch: 18, Validation Loss: 1.6135\n",
      "Epoch: 19, Training Loss: 1.3358\n",
      "Epoch: 19, Validation Loss: 1.4939\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491f89fe5f9f4a448f6267595353e86b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃▄▂▅▄▃▃▆▃▂▂▃▅▂▃▂▁▂█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>1.33585</td></tr><tr><td>val_loss</td><td>1.49388</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-4</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/fw2sbetj\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/fw2sbetj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_171703-fw2sbetj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zuyrz62r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0408873216422968\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: mean\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_174120-zuyrz62r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/zuyrz62r\" target=\"_blank\">revived-sweep-5</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/zuyrz62r\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/zuyrz62r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=mean, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.6427\n",
      "Epoch: 0, Validation Loss: 1.2451\n",
      "Epoch: 1, Training Loss: 1.2850\n",
      "Epoch: 1, Validation Loss: 1.2673\n",
      "Epoch: 2, Training Loss: 1.2871\n",
      "Epoch: 2, Validation Loss: 1.2558\n",
      "Epoch: 3, Training Loss: 1.2865\n",
      "Epoch: 3, Validation Loss: 1.3542\n",
      "Epoch: 4, Training Loss: 1.3402\n",
      "Epoch: 4, Validation Loss: 1.2575\n",
      "Epoch: 5, Training Loss: 1.3351\n",
      "Epoch: 5, Validation Loss: 1.4335\n",
      "Epoch: 6, Training Loss: 1.3236\n",
      "Epoch: 6, Validation Loss: 1.3236\n",
      "Epoch: 7, Training Loss: 1.3508\n",
      "Epoch: 7, Validation Loss: 1.3565\n",
      "Epoch: 8, Training Loss: 1.3265\n",
      "Epoch: 8, Validation Loss: 1.2826\n",
      "Epoch: 9, Training Loss: 1.3076\n",
      "Epoch: 9, Validation Loss: 1.2964\n",
      "Epoch: 10, Training Loss: 1.3014\n",
      "Epoch: 10, Validation Loss: 1.3429\n",
      "Epoch: 11, Training Loss: 1.3080\n",
      "Epoch: 11, Validation Loss: 1.2710\n",
      "Epoch: 12, Training Loss: 1.3151\n",
      "Epoch: 12, Validation Loss: 1.3244\n",
      "Epoch: 13, Training Loss: 1.3275\n",
      "Epoch: 13, Validation Loss: 1.3349\n",
      "Epoch: 14, Training Loss: 1.3324\n",
      "Epoch: 14, Validation Loss: 1.3096\n",
      "Epoch: 15, Training Loss: 1.3177\n",
      "Epoch: 15, Validation Loss: 1.2797\n",
      "Epoch: 16, Training Loss: 1.3062\n",
      "Epoch: 16, Validation Loss: 1.2758\n",
      "Epoch: 17, Training Loss: 1.3012\n",
      "Epoch: 17, Validation Loss: 1.3040\n",
      "Epoch: 18, Training Loss: 1.3065\n",
      "Epoch: 18, Validation Loss: 1.3692\n",
      "Epoch: 19, Training Loss: 1.3022\n",
      "Epoch: 19, Validation Loss: 1.3287\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▁▁▁▂▂▂▂▂▁▁▁▂▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▂▁▅▁█▄▅▂▃▅▂▄▄▃▂▂▃▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>1.30225</td></tr><tr><td>val_loss</td><td>1.32868</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-sweep-5</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/zuyrz62r\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/zuyrz62r</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_174120-zuyrz62r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ppze1tmw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 80\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.03664328677093611\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: mean\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_180217-ppze1tmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ppze1tmw\" target=\"_blank\">quiet-sweep-6</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ppze1tmw\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ppze1tmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=mean, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.7219\n",
      "Epoch: 0, Validation Loss: 1.3601\n",
      "Epoch: 1, Training Loss: 1.2750\n",
      "Epoch: 1, Validation Loss: 1.2603\n",
      "Epoch: 2, Training Loss: 1.2673\n",
      "Epoch: 2, Validation Loss: 1.2556\n",
      "Epoch: 3, Training Loss: 1.2613\n",
      "Epoch: 3, Validation Loss: 1.2473\n",
      "Epoch: 4, Training Loss: 1.2527\n",
      "Epoch: 4, Validation Loss: 1.2499\n",
      "Epoch: 5, Training Loss: 1.2545\n",
      "Epoch: 5, Validation Loss: 1.2864\n",
      "Epoch: 6, Training Loss: 1.2596\n",
      "Epoch: 6, Validation Loss: 1.2932\n",
      "Epoch: 7, Training Loss: 1.2641\n",
      "Epoch: 7, Validation Loss: 1.2376\n",
      "Epoch: 8, Training Loss: 1.2565\n",
      "Epoch: 8, Validation Loss: 1.2910\n",
      "Epoch: 9, Training Loss: 1.2619\n",
      "Epoch: 9, Validation Loss: 1.2554\n",
      "Epoch: 10, Training Loss: 1.2558\n",
      "Epoch: 10, Validation Loss: 1.2799\n",
      "Epoch: 11, Training Loss: 1.2605\n",
      "Epoch: 11, Validation Loss: 1.2471\n",
      "Epoch: 12, Training Loss: 1.2605\n",
      "Epoch: 12, Validation Loss: 1.2510\n",
      "Epoch: 13, Training Loss: 1.2584\n",
      "Epoch: 13, Validation Loss: 1.2376\n",
      "Epoch: 14, Training Loss: 1.2583\n",
      "Epoch: 14, Validation Loss: 1.2400\n",
      "Epoch: 15, Training Loss: 1.2551\n",
      "Epoch: 15, Validation Loss: 1.2552\n",
      "Epoch: 16, Training Loss: 1.2586\n",
      "Epoch: 16, Validation Loss: 1.2569\n",
      "Epoch: 17, Training Loss: 1.2558\n",
      "Epoch: 17, Validation Loss: 1.2763\n",
      "Epoch: 18, Training Loss: 1.2664\n",
      "Epoch: 18, Validation Loss: 1.4365\n",
      "Epoch: 19, Training Loss: 1.3124\n",
      "Epoch: 19, Validation Loss: 1.3059\n",
      "Epoch: 20, Training Loss: 1.3297\n",
      "Epoch: 20, Validation Loss: 1.2804\n",
      "Epoch: 21, Training Loss: 1.3052\n",
      "Epoch: 21, Validation Loss: 1.3269\n",
      "Epoch: 22, Training Loss: 1.2792\n",
      "Epoch: 22, Validation Loss: 1.3059\n",
      "Epoch: 23, Training Loss: 1.2888\n",
      "Epoch: 23, Validation Loss: 1.2823\n",
      "Epoch: 24, Training Loss: 1.2770\n",
      "Epoch: 24, Validation Loss: 1.2965\n",
      "Epoch: 25, Training Loss: 1.2905\n",
      "Epoch: 25, Validation Loss: 1.3071\n",
      "Epoch: 26, Training Loss: 1.2749\n",
      "Epoch: 26, Validation Loss: 1.3041\n",
      "Epoch: 27, Training Loss: 1.2831\n",
      "Epoch: 27, Validation Loss: 1.2679\n",
      "Epoch: 28, Training Loss: 1.2873\n",
      "Epoch: 28, Validation Loss: 1.2965\n",
      "Epoch: 29, Training Loss: 1.2796\n",
      "Epoch: 29, Validation Loss: 1.2888\n",
      "Epoch: 30, Training Loss: 1.2681\n",
      "Epoch: 30, Validation Loss: 1.2638\n",
      "Epoch: 31, Training Loss: 1.2791\n",
      "Epoch: 31, Validation Loss: 1.2786\n",
      "Epoch: 32, Training Loss: 1.2714\n",
      "Epoch: 32, Validation Loss: 1.2753\n",
      "Epoch: 33, Training Loss: 1.2724\n",
      "Epoch: 33, Validation Loss: 1.2572\n",
      "Epoch: 34, Training Loss: 1.4376\n",
      "Epoch: 34, Validation Loss: 1.4678\n",
      "Epoch: 35, Training Loss: 1.4813\n",
      "Epoch: 35, Validation Loss: 1.4718\n",
      "Epoch: 36, Training Loss: 1.4742\n",
      "Epoch: 36, Validation Loss: 1.4645\n",
      "Epoch: 37, Training Loss: 1.4787\n",
      "Epoch: 37, Validation Loss: 1.4783\n",
      "Epoch: 38, Training Loss: 1.4700\n",
      "Epoch: 38, Validation Loss: 1.4662\n",
      "Epoch: 39, Training Loss: 1.4721\n",
      "Epoch: 39, Validation Loss: 1.4746\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da799c8998b84e3db97ac02c94df6331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▄▄▄▄▄▄</td></tr><tr><td>val_loss</td><td>▅▂▂▁▁▂▃▁▃▂▂▁▁▁▁▂▂▂▇▃▂▄▃▂▃▃▃▂▃▂▂▂▂▂██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>1.47209</td></tr><tr><td>val_loss</td><td>1.47459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">quiet-sweep-6</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ppze1tmw\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ppze1tmw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_180217-ppze1tmw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gqv9kfwq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.056185635237460965\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_183417-gqv9kfwq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/gqv9kfwq\" target=\"_blank\">deep-sweep-7</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/gqv9kfwq\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/gqv9kfwq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=max, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 2.2305\n",
      "Epoch: 0, Validation Loss: 1.5036\n",
      "Epoch: 1, Training Loss: 1.4888\n",
      "Epoch: 1, Validation Loss: 1.4837\n",
      "Epoch: 2, Training Loss: 1.8984\n",
      "Epoch: 2, Validation Loss: 2.0765\n",
      "Epoch: 3, Training Loss: 2.0681\n",
      "Epoch: 3, Validation Loss: 2.0776\n",
      "Epoch: 4, Training Loss: 2.0668\n",
      "Epoch: 4, Validation Loss: 2.0773\n",
      "Epoch: 5, Training Loss: 2.0672\n",
      "Epoch: 5, Validation Loss: 2.0766\n",
      "Epoch: 6, Training Loss: 2.0668\n",
      "Epoch: 6, Validation Loss: 2.0770\n",
      "Epoch: 7, Training Loss: 2.0673\n",
      "Epoch: 7, Validation Loss: 2.0773\n",
      "Epoch: 8, Training Loss: 2.0668\n",
      "Epoch: 8, Validation Loss: 2.0824\n",
      "Epoch: 9, Training Loss: 2.0670\n",
      "Epoch: 9, Validation Loss: 2.0913\n",
      "Epoch: 10, Training Loss: 2.0668\n",
      "Epoch: 10, Validation Loss: 2.0857\n",
      "Epoch: 11, Training Loss: 2.0666\n",
      "Epoch: 11, Validation Loss: 2.0836\n",
      "Epoch: 12, Training Loss: 2.0670\n",
      "Epoch: 12, Validation Loss: 2.0868\n",
      "Epoch: 13, Training Loss: 2.0666\n",
      "Epoch: 13, Validation Loss: 2.0769\n",
      "Epoch: 14, Training Loss: 2.0676\n",
      "Epoch: 14, Validation Loss: 2.0766\n",
      "Epoch: 15, Training Loss: 2.0662\n",
      "Epoch: 15, Validation Loss: 2.0902\n",
      "Epoch: 16, Training Loss: 2.0672\n",
      "Epoch: 16, Validation Loss: 2.0812\n",
      "Epoch: 17, Training Loss: 2.0664\n",
      "Epoch: 17, Validation Loss: 2.0806\n",
      "Epoch: 18, Training Loss: 2.0671\n",
      "Epoch: 18, Validation Loss: 2.0884\n",
      "Epoch: 19, Training Loss: 2.0666\n",
      "Epoch: 19, Validation Loss: 2.0771\n",
      "Epoch: 20, Training Loss: 2.0657\n",
      "Epoch: 20, Validation Loss: 2.0871\n",
      "Epoch: 21, Training Loss: 2.0663\n",
      "Epoch: 21, Validation Loss: 2.0966\n",
      "Epoch: 22, Training Loss: 2.0669\n",
      "Epoch: 22, Validation Loss: 2.0794\n",
      "Epoch: 23, Training Loss: 2.0677\n",
      "Epoch: 23, Validation Loss: 2.0883\n",
      "Epoch: 24, Training Loss: 2.0658\n",
      "Epoch: 24, Validation Loss: 2.0862\n",
      "Epoch: 25, Training Loss: 2.0673\n",
      "Epoch: 25, Validation Loss: 2.0777\n",
      "Epoch: 26, Training Loss: 2.0667\n",
      "Epoch: 26, Validation Loss: 2.0873\n",
      "Epoch: 27, Training Loss: 2.0665\n",
      "Epoch: 27, Validation Loss: 2.0784\n",
      "Epoch: 28, Training Loss: 2.0672\n",
      "Epoch: 28, Validation Loss: 2.0777\n",
      "Epoch: 29, Training Loss: 2.0670\n",
      "Epoch: 29, Validation Loss: 2.1627\n",
      "Epoch: 30, Training Loss: 2.0663\n",
      "Epoch: 30, Validation Loss: 2.0796\n",
      "Epoch: 31, Training Loss: 2.0674\n",
      "Epoch: 31, Validation Loss: 2.1275\n",
      "Epoch: 32, Training Loss: 2.0676\n",
      "Epoch: 32, Validation Loss: 2.0905\n",
      "Epoch: 33, Training Loss: 2.0671\n",
      "Epoch: 33, Validation Loss: 2.0775\n",
      "Epoch: 34, Training Loss: 2.0671\n",
      "Epoch: 34, Validation Loss: 2.0776\n",
      "Epoch: 35, Training Loss: 2.0660\n",
      "Epoch: 35, Validation Loss: 2.0808\n",
      "Epoch: 36, Training Loss: 2.0674\n",
      "Epoch: 36, Validation Loss: 2.0822\n",
      "Epoch: 37, Training Loss: 2.0684\n",
      "Epoch: 37, Validation Loss: 2.0895\n",
      "Epoch: 38, Training Loss: 2.0667\n",
      "Epoch: 38, Validation Loss: 2.0764\n",
      "Epoch: 39, Training Loss: 2.0664\n",
      "Epoch: 39, Validation Loss: 2.0816\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520cecb57da64471813e0b5d843128eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.016 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.040030…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▅▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>val_loss</td><td>▁▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>2.06644</td></tr><tr><td>val_loss</td><td>2.08158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-sweep-7</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/gqv9kfwq\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/gqv9kfwq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_183417-gqv9kfwq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ys0m3tn2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.09339342816746486\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: add\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_192210-ys0m3tn2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ys0m3tn2\" target=\"_blank\">leafy-sweep-8</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ys0m3tn2\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ys0m3tn2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=add, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 125.6813\n",
      "Epoch: 0, Validation Loss: 1.3347\n",
      "Epoch: 1, Training Loss: 1.4418\n",
      "Epoch: 1, Validation Loss: 1.4979\n",
      "Epoch: 2, Training Loss: 1.4802\n",
      "Epoch: 2, Validation Loss: 1.4764\n",
      "Epoch: 3, Training Loss: 1.4762\n",
      "Epoch: 3, Validation Loss: 1.5206\n",
      "Epoch: 4, Training Loss: 1.4750\n",
      "Epoch: 4, Validation Loss: 1.5400\n",
      "Epoch: 5, Training Loss: 1.4652\n",
      "Epoch: 5, Validation Loss: 1.4420\n",
      "Epoch: 6, Training Loss: 1.4807\n",
      "Epoch: 6, Validation Loss: 1.4126\n",
      "Epoch: 7, Training Loss: 1.4835\n",
      "Epoch: 7, Validation Loss: 1.7394\n",
      "Epoch: 8, Training Loss: 1.4799\n",
      "Epoch: 8, Validation Loss: 1.4520\n",
      "Epoch: 9, Training Loss: 1.4907\n",
      "Epoch: 9, Validation Loss: 1.4293\n",
      "Epoch: 10, Training Loss: 1.4761\n",
      "Epoch: 10, Validation Loss: 1.4351\n",
      "Epoch: 11, Training Loss: 1.4822\n",
      "Epoch: 11, Validation Loss: 1.4219\n",
      "Epoch: 12, Training Loss: 1.4723\n",
      "Epoch: 12, Validation Loss: 1.6201\n",
      "Epoch: 13, Training Loss: 1.4754\n",
      "Epoch: 13, Validation Loss: 1.4517\n",
      "Epoch: 14, Training Loss: 1.4721\n",
      "Epoch: 14, Validation Loss: 1.5246\n",
      "Epoch: 15, Training Loss: 1.4600\n",
      "Epoch: 15, Validation Loss: 1.3887\n",
      "Epoch: 16, Training Loss: 1.4910\n",
      "Epoch: 16, Validation Loss: 1.4201\n",
      "Epoch: 17, Training Loss: 1.4762\n",
      "Epoch: 17, Validation Loss: 1.4584\n",
      "Epoch: 18, Training Loss: 1.4671\n",
      "Epoch: 18, Validation Loss: 1.8541\n",
      "Epoch: 19, Training Loss: 1.4821\n",
      "Epoch: 19, Validation Loss: 1.7624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1917689127984b8db0e6522a09f05086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.043531…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▃▃▄▄▂▂▆▃▂▂▂▅▃▄▂▂▃█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>1.48212</td></tr><tr><td>val_loss</td><td>1.76238</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-sweep-8</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ys0m3tn2\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/ys0m3tn2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_192210-ys0m3tn2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dig1s334 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.05422102063619725\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: mean\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_194304-dig1s334</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dig1s334\" target=\"_blank\">decent-sweep-9</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dig1s334\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dig1s334</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=mean, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 2.2125\n",
      "Epoch: 0, Validation Loss: 1.2570\n",
      "Epoch: 1, Training Loss: 1.3019\n",
      "Epoch: 1, Validation Loss: 1.3583\n",
      "Epoch: 2, Training Loss: 1.3103\n",
      "Epoch: 2, Validation Loss: 1.2829\n",
      "Epoch: 3, Training Loss: 1.3324\n",
      "Epoch: 3, Validation Loss: 1.4148\n",
      "Epoch: 4, Training Loss: 1.3644\n",
      "Epoch: 4, Validation Loss: 1.3233\n",
      "Epoch: 5, Training Loss: 1.3307\n",
      "Epoch: 5, Validation Loss: 1.3824\n",
      "Epoch: 6, Training Loss: 1.3179\n",
      "Epoch: 6, Validation Loss: 1.2788\n",
      "Epoch: 7, Training Loss: 1.3184\n",
      "Epoch: 7, Validation Loss: 1.3121\n",
      "Epoch: 8, Training Loss: 1.3255\n",
      "Epoch: 8, Validation Loss: 1.3828\n",
      "Epoch: 9, Training Loss: 1.3127\n",
      "Epoch: 9, Validation Loss: 1.2875\n",
      "Epoch: 10, Training Loss: 1.3109\n",
      "Epoch: 10, Validation Loss: 1.2881\n",
      "Epoch: 11, Training Loss: 1.3150\n",
      "Epoch: 11, Validation Loss: 1.2807\n",
      "Epoch: 12, Training Loss: 1.3125\n",
      "Epoch: 12, Validation Loss: 1.2987\n",
      "Epoch: 13, Training Loss: 1.3164\n",
      "Epoch: 13, Validation Loss: 1.2849\n",
      "Epoch: 14, Training Loss: 1.3149\n",
      "Epoch: 14, Validation Loss: 1.3474\n",
      "Epoch: 15, Training Loss: 1.3129\n",
      "Epoch: 15, Validation Loss: 1.2649\n",
      "Epoch: 16, Training Loss: 1.3129\n",
      "Epoch: 16, Validation Loss: 1.2965\n",
      "Epoch: 17, Training Loss: 1.3132\n",
      "Epoch: 17, Validation Loss: 1.2929\n",
      "Epoch: 18, Training Loss: 1.3075\n",
      "Epoch: 18, Validation Loss: 1.3117\n",
      "Epoch: 19, Training Loss: 1.3083\n",
      "Epoch: 19, Validation Loss: 1.2782\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▅▂█▄▇▂▃▇▂▂▂▃▂▅▁▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>1.30829</td></tr><tr><td>val_loss</td><td>1.2782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-9</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dig1s334\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dig1s334</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_194304-dig1s334/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 208evql4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 104\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.027723069561506844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988dbcb6a74d41218d68db232cb4ef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016701001666660886, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_200246-208evql4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/208evql4\" target=\"_blank\">scarlet-sweep-10</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/208evql4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/208evql4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=max, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.6081\n",
      "Epoch: 0, Validation Loss: 1.3208\n",
      "Epoch: 1, Training Loss: 1.3193\n",
      "Epoch: 1, Validation Loss: 1.3231\n",
      "Epoch: 2, Training Loss: 1.3019\n",
      "Epoch: 2, Validation Loss: 1.3357\n",
      "Epoch: 3, Training Loss: 1.2976\n",
      "Epoch: 3, Validation Loss: 1.3016\n",
      "Epoch: 4, Training Loss: 1.2986\n",
      "Epoch: 4, Validation Loss: 1.3262\n",
      "Epoch: 5, Training Loss: 1.2958\n",
      "Epoch: 5, Validation Loss: 1.3158\n",
      "Epoch: 6, Training Loss: 1.3027\n",
      "Epoch: 6, Validation Loss: 1.3251\n",
      "Epoch: 7, Training Loss: 1.2969\n",
      "Epoch: 7, Validation Loss: 1.3085\n",
      "Epoch: 8, Training Loss: 1.2952\n",
      "Epoch: 8, Validation Loss: 1.2914\n",
      "Epoch: 9, Training Loss: 1.2978\n",
      "Epoch: 9, Validation Loss: 1.2999\n",
      "Epoch: 10, Training Loss: 1.2940\n",
      "Epoch: 10, Validation Loss: 1.3171\n",
      "Epoch: 11, Training Loss: 1.2953\n",
      "Epoch: 11, Validation Loss: 1.2935\n",
      "Epoch: 12, Training Loss: 1.2954\n",
      "Epoch: 12, Validation Loss: 1.3081\n",
      "Epoch: 13, Training Loss: 1.2906\n",
      "Epoch: 13, Validation Loss: 1.3041\n",
      "Epoch: 14, Training Loss: 1.2980\n",
      "Epoch: 14, Validation Loss: 1.3152\n",
      "Epoch: 15, Training Loss: 1.3032\n",
      "Epoch: 15, Validation Loss: 1.3170\n",
      "Epoch: 16, Training Loss: 1.2953\n",
      "Epoch: 16, Validation Loss: 1.3271\n",
      "Epoch: 17, Training Loss: 1.2929\n",
      "Epoch: 17, Validation Loss: 1.3041\n",
      "Epoch: 18, Training Loss: 1.2959\n",
      "Epoch: 18, Validation Loss: 1.3338\n",
      "Epoch: 19, Training Loss: 1.2907\n",
      "Epoch: 19, Validation Loss: 1.2892\n",
      "Epoch: 20, Training Loss: 1.2895\n",
      "Epoch: 20, Validation Loss: 1.3129\n",
      "Epoch: 21, Training Loss: 1.2978\n",
      "Epoch: 21, Validation Loss: 1.3262\n",
      "Epoch: 22, Training Loss: 1.2905\n",
      "Epoch: 22, Validation Loss: 1.3071\n",
      "Epoch: 23, Training Loss: 1.2973\n",
      "Epoch: 23, Validation Loss: 1.3153\n",
      "Epoch: 24, Training Loss: 1.2945\n",
      "Epoch: 24, Validation Loss: 1.3820\n",
      "Epoch: 25, Training Loss: 1.2919\n",
      "Epoch: 25, Validation Loss: 1.3310\n",
      "Epoch: 26, Training Loss: 1.2897\n",
      "Epoch: 26, Validation Loss: 1.3217\n",
      "Epoch: 27, Training Loss: 1.3193\n",
      "Epoch: 27, Validation Loss: 1.3124\n",
      "Epoch: 28, Training Loss: 1.3147\n",
      "Epoch: 28, Validation Loss: 1.3275\n",
      "Epoch: 29, Training Loss: 1.3122\n",
      "Epoch: 29, Validation Loss: 1.4322\n",
      "Epoch: 30, Training Loss: 1.2982\n",
      "Epoch: 30, Validation Loss: 1.3001\n",
      "Epoch: 31, Training Loss: 1.2981\n",
      "Epoch: 31, Validation Loss: 1.3815\n",
      "Epoch: 32, Training Loss: 1.2976\n",
      "Epoch: 32, Validation Loss: 1.2956\n",
      "Epoch: 33, Training Loss: 1.2990\n",
      "Epoch: 33, Validation Loss: 1.3065\n",
      "Epoch: 34, Training Loss: 1.3010\n",
      "Epoch: 34, Validation Loss: 1.3024\n",
      "Epoch: 35, Training Loss: 1.2978\n",
      "Epoch: 35, Validation Loss: 1.3033\n",
      "Epoch: 36, Training Loss: 1.2952\n",
      "Epoch: 36, Validation Loss: 1.3661\n",
      "Epoch: 37, Training Loss: 1.3009\n",
      "Epoch: 37, Validation Loss: 1.3120\n",
      "Epoch: 38, Training Loss: 1.2970\n",
      "Epoch: 38, Validation Loss: 1.2883\n",
      "Epoch: 39, Training Loss: 1.2959\n",
      "Epoch: 39, Validation Loss: 1.2919\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1c308ea05d4ab993c42e2b9e71873c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.016 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.040085…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▃▃▃▂▃▂▃▂▁▂▂▁▂▂▂▂▃▂▃▁▂▃▂▂▆▃▃▂▃█▂▆▁▂▂▂▅▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>1.29594</td></tr><tr><td>val_loss</td><td>1.29195</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">scarlet-sweep-10</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/208evql4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/208evql4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_200246-208evql4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: anuxpcgf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.021471252059655967\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: mean\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_203331-anuxpcgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/anuxpcgf\" target=\"_blank\">valiant-sweep-11</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/anuxpcgf\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/anuxpcgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=mean, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.3252\n",
      "Epoch: 0, Validation Loss: 1.2526\n",
      "Epoch: 1, Training Loss: 1.2425\n",
      "Epoch: 1, Validation Loss: 1.2564\n",
      "Epoch: 2, Training Loss: 1.2253\n",
      "Epoch: 2, Validation Loss: 1.1755\n",
      "Epoch: 3, Training Loss: 1.1799\n",
      "Epoch: 3, Validation Loss: 1.1587\n",
      "Epoch: 4, Training Loss: 1.1582\n",
      "Epoch: 4, Validation Loss: 1.1221\n",
      "Epoch: 5, Training Loss: 1.1478\n",
      "Epoch: 5, Validation Loss: 1.1869\n",
      "Epoch: 6, Training Loss: 1.1308\n",
      "Epoch: 6, Validation Loss: 1.1083\n",
      "Epoch: 7, Training Loss: 1.1314\n",
      "Epoch: 7, Validation Loss: 1.1214\n",
      "Epoch: 8, Training Loss: 1.1399\n",
      "Epoch: 8, Validation Loss: 1.1931\n",
      "Epoch: 9, Training Loss: 1.1328\n",
      "Epoch: 9, Validation Loss: 1.1430\n",
      "Epoch: 10, Training Loss: 1.1281\n",
      "Epoch: 10, Validation Loss: 1.0806\n",
      "Epoch: 11, Training Loss: 1.1208\n",
      "Epoch: 11, Validation Loss: 1.1180\n",
      "Epoch: 12, Training Loss: 1.1194\n",
      "Epoch: 12, Validation Loss: 1.1036\n",
      "Epoch: 13, Training Loss: 1.1208\n",
      "Epoch: 13, Validation Loss: 1.0904\n",
      "Epoch: 14, Training Loss: 1.1236\n",
      "Epoch: 14, Validation Loss: 1.1126\n",
      "Epoch: 15, Training Loss: 1.1217\n",
      "Epoch: 15, Validation Loss: 1.0877\n",
      "Epoch: 16, Training Loss: 1.1163\n",
      "Epoch: 16, Validation Loss: 1.0822\n",
      "Epoch: 17, Training Loss: 1.1163\n",
      "Epoch: 17, Validation Loss: 1.1230\n",
      "Epoch: 18, Training Loss: 1.1297\n",
      "Epoch: 18, Validation Loss: 1.1297\n",
      "Epoch: 19, Training Loss: 1.1191\n",
      "Epoch: 19, Validation Loss: 1.1059\n",
      "Epoch: 20, Training Loss: 1.1203\n",
      "Epoch: 20, Validation Loss: 1.0961\n",
      "Epoch: 21, Training Loss: 1.1143\n",
      "Epoch: 21, Validation Loss: 1.0820\n",
      "Epoch: 22, Training Loss: 1.1169\n",
      "Epoch: 22, Validation Loss: 1.1299\n",
      "Epoch: 23, Training Loss: 1.1238\n",
      "Epoch: 23, Validation Loss: 1.0968\n",
      "Epoch: 24, Training Loss: 1.1186\n",
      "Epoch: 24, Validation Loss: 1.0834\n",
      "Epoch: 25, Training Loss: 1.1158\n",
      "Epoch: 25, Validation Loss: 1.1514\n",
      "Epoch: 26, Training Loss: 1.1212\n",
      "Epoch: 26, Validation Loss: 1.1145\n",
      "Epoch: 27, Training Loss: 1.1191\n",
      "Epoch: 27, Validation Loss: 1.0879\n",
      "Epoch: 28, Training Loss: 1.1106\n",
      "Epoch: 28, Validation Loss: 1.0806\n",
      "Epoch: 29, Training Loss: 1.1098\n",
      "Epoch: 29, Validation Loss: 1.0862\n",
      "Epoch: 30, Training Loss: 1.1238\n",
      "Epoch: 30, Validation Loss: 1.1783\n",
      "Epoch: 31, Training Loss: 1.1093\n",
      "Epoch: 31, Validation Loss: 1.1880\n",
      "Epoch: 32, Training Loss: 1.1083\n",
      "Epoch: 32, Validation Loss: 1.1156\n",
      "Epoch: 33, Training Loss: 1.1143\n",
      "Epoch: 33, Validation Loss: 1.0780\n",
      "Epoch: 34, Training Loss: 1.1169\n",
      "Epoch: 34, Validation Loss: 1.0796\n",
      "Epoch: 35, Training Loss: 1.1112\n",
      "Epoch: 35, Validation Loss: 1.0814\n",
      "Epoch: 36, Training Loss: 1.1116\n",
      "Epoch: 36, Validation Loss: 1.1749\n",
      "Epoch: 37, Training Loss: 1.1111\n",
      "Epoch: 37, Validation Loss: 1.2452\n",
      "Epoch: 38, Training Loss: 1.1161\n",
      "Epoch: 38, Validation Loss: 1.0938\n",
      "Epoch: 39, Training Loss: 1.1145\n",
      "Epoch: 39, Validation Loss: 1.1058\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea1add1d5664ebea4ea89ae95db4512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▅▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▅▄▃▅▂▃▆▄▁▃▂▁▂▁▁▃▃▂▂▁▃▂▁▄▂▁▁▁▅▅▂▁▁▁▅█▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>1.11454</td></tr><tr><td>val_loss</td><td>1.10581</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">valiant-sweep-11</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/anuxpcgf\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/anuxpcgf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_203331-anuxpcgf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dvu7nezt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0018025981435516591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_211007-dvu7nezt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dvu7nezt\" target=\"_blank\">smooth-sweep-12</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dvu7nezt\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dvu7nezt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=max, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.2937\n",
      "Epoch: 0, Validation Loss: 1.2385\n",
      "Epoch: 1, Training Loss: 1.1591\n",
      "Epoch: 1, Validation Loss: 1.0299\n",
      "Epoch: 2, Training Loss: 1.0729\n",
      "Epoch: 2, Validation Loss: 1.0070\n",
      "Epoch: 3, Training Loss: 1.0369\n",
      "Epoch: 3, Validation Loss: 1.0286\n",
      "Epoch: 4, Training Loss: 1.0269\n",
      "Epoch: 4, Validation Loss: 0.9993\n",
      "Epoch: 5, Training Loss: 1.0091\n",
      "Epoch: 5, Validation Loss: 1.0049\n",
      "Epoch: 6, Training Loss: 0.9944\n",
      "Epoch: 6, Validation Loss: 0.9744\n",
      "Epoch: 7, Training Loss: 0.9824\n",
      "Epoch: 7, Validation Loss: 0.9792\n",
      "Epoch: 8, Training Loss: 0.9771\n",
      "Epoch: 8, Validation Loss: 0.9392\n",
      "Epoch: 9, Training Loss: 0.9682\n",
      "Epoch: 9, Validation Loss: 0.9473\n",
      "Epoch: 10, Training Loss: 0.9618\n",
      "Epoch: 10, Validation Loss: 0.9508\n",
      "Epoch: 11, Training Loss: 0.9579\n",
      "Epoch: 11, Validation Loss: 1.0090\n",
      "Epoch: 12, Training Loss: 0.9605\n",
      "Epoch: 12, Validation Loss: 0.9665\n",
      "Epoch: 13, Training Loss: 0.9490\n",
      "Epoch: 13, Validation Loss: 0.9718\n",
      "Epoch: 14, Training Loss: 0.9484\n",
      "Epoch: 14, Validation Loss: 0.9440\n",
      "Epoch: 15, Training Loss: 0.9461\n",
      "Epoch: 15, Validation Loss: 0.9335\n",
      "Epoch: 16, Training Loss: 0.9464\n",
      "Epoch: 16, Validation Loss: 0.9233\n",
      "Epoch: 17, Training Loss: 0.9403\n",
      "Epoch: 17, Validation Loss: 0.9441\n",
      "Epoch: 18, Training Loss: 0.9399\n",
      "Epoch: 18, Validation Loss: 0.9459\n",
      "Epoch: 19, Training Loss: 0.9371\n",
      "Epoch: 19, Validation Loss: 0.9431\n",
      "Epoch: 20, Training Loss: 0.9326\n",
      "Epoch: 20, Validation Loss: 0.9340\n",
      "Epoch: 21, Training Loss: 0.9275\n",
      "Epoch: 21, Validation Loss: 0.9116\n",
      "Epoch: 22, Training Loss: 0.9278\n",
      "Epoch: 22, Validation Loss: 0.9225\n",
      "Epoch: 23, Training Loss: 0.9274\n",
      "Epoch: 23, Validation Loss: 0.9366\n",
      "Epoch: 24, Training Loss: 0.9251\n",
      "Epoch: 24, Validation Loss: 0.9201\n",
      "Epoch: 25, Training Loss: 0.9212\n",
      "Epoch: 25, Validation Loss: 0.9071\n",
      "Epoch: 26, Training Loss: 0.9234\n",
      "Epoch: 26, Validation Loss: 0.9006\n",
      "Epoch: 27, Training Loss: 0.9203\n",
      "Epoch: 27, Validation Loss: 0.8873\n",
      "Epoch: 28, Training Loss: 0.9163\n",
      "Epoch: 28, Validation Loss: 0.9107\n",
      "Epoch: 29, Training Loss: 0.9153\n",
      "Epoch: 29, Validation Loss: 0.9371\n",
      "Epoch: 30, Training Loss: 0.9170\n",
      "Epoch: 30, Validation Loss: 0.9561\n",
      "Epoch: 31, Training Loss: 0.9149\n",
      "Epoch: 31, Validation Loss: 0.9433\n",
      "Epoch: 32, Training Loss: 0.9131\n",
      "Epoch: 32, Validation Loss: 0.9556\n",
      "Epoch: 33, Training Loss: 0.9119\n",
      "Epoch: 33, Validation Loss: 0.9107\n",
      "Epoch: 34, Training Loss: 0.9096\n",
      "Epoch: 34, Validation Loss: 0.9119\n",
      "Epoch: 35, Training Loss: 0.9104\n",
      "Epoch: 35, Validation Loss: 0.9025\n",
      "Epoch: 36, Training Loss: 0.9088\n",
      "Epoch: 36, Validation Loss: 0.8964\n",
      "Epoch: 37, Training Loss: 0.9055\n",
      "Epoch: 37, Validation Loss: 0.9148\n",
      "Epoch: 38, Training Loss: 0.9080\n",
      "Epoch: 38, Validation Loss: 0.9096\n",
      "Epoch: 39, Training Loss: 0.9047\n",
      "Epoch: 39, Validation Loss: 0.8988\n",
      "Epoch: 40, Training Loss: 0.9069\n",
      "Epoch: 40, Validation Loss: 0.9131\n",
      "Epoch: 41, Training Loss: 0.9052\n",
      "Epoch: 41, Validation Loss: 0.9707\n",
      "Epoch: 42, Training Loss: 0.9034\n",
      "Epoch: 42, Validation Loss: 0.8932\n",
      "Epoch: 43, Training Loss: 0.9020\n",
      "Epoch: 43, Validation Loss: 0.8880\n",
      "Epoch: 44, Training Loss: 0.9069\n",
      "Epoch: 44, Validation Loss: 0.9012\n",
      "Epoch: 45, Training Loss: 0.8979\n",
      "Epoch: 45, Validation Loss: 0.9254\n",
      "Epoch: 46, Training Loss: 0.8986\n",
      "Epoch: 46, Validation Loss: 0.9019\n",
      "Epoch: 47, Training Loss: 0.9028\n",
      "Epoch: 47, Validation Loss: 0.8948\n",
      "Epoch: 48, Training Loss: 0.8973\n",
      "Epoch: 48, Validation Loss: 0.9371\n",
      "Epoch: 49, Training Loss: 0.9016\n",
      "Epoch: 49, Validation Loss: 0.8906\n",
      "Epoch: 50, Training Loss: 0.8978\n",
      "Epoch: 50, Validation Loss: 0.8977\n",
      "Epoch: 51, Training Loss: 0.8936\n",
      "Epoch: 51, Validation Loss: 0.8865\n",
      "Epoch: 52, Training Loss: 0.8924\n",
      "Epoch: 52, Validation Loss: 0.9123\n",
      "Epoch: 53, Training Loss: 0.8942\n",
      "Epoch: 53, Validation Loss: 0.8877\n",
      "Epoch: 54, Training Loss: 0.8944\n",
      "Epoch: 54, Validation Loss: 0.9212\n",
      "Epoch: 55, Training Loss: 0.8933\n",
      "Epoch: 55, Validation Loss: 0.8852\n",
      "Epoch: 56, Training Loss: 0.8890\n",
      "Epoch: 56, Validation Loss: 0.9355\n",
      "Epoch: 57, Training Loss: 0.8923\n",
      "Epoch: 57, Validation Loss: 0.9017\n",
      "Epoch: 58, Training Loss: 0.8959\n",
      "Epoch: 58, Validation Loss: 0.8969\n",
      "Epoch: 59, Training Loss: 0.8910\n",
      "Epoch: 59, Validation Loss: 0.9161\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fbbffa18a3f4148b4ea78d94f39919d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.017 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.037097…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▄▃▃▃▂▂▃▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▁▂▁▂▁▁▂▁▂▁▁▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>59</td></tr><tr><td>train_loss</td><td>0.89098</td></tr><tr><td>val_loss</td><td>0.91612</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smooth-sweep-12</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dvu7nezt\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/dvu7nezt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_211007-dvu7nezt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u71imis3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 48\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 60\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.018825446947448456\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: add\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_222250-u71imis3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/u71imis3\" target=\"_blank\">efficient-sweep-13</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/u71imis3\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/u71imis3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=add, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.4941\n",
      "Epoch: 0, Validation Loss: 1.2481\n",
      "Epoch: 1, Training Loss: 1.2396\n",
      "Epoch: 1, Validation Loss: 1.1841\n",
      "Epoch: 2, Training Loss: 1.1049\n",
      "Epoch: 2, Validation Loss: 1.0638\n",
      "Epoch: 3, Training Loss: 1.0886\n",
      "Epoch: 3, Validation Loss: 1.0757\n",
      "Epoch: 4, Training Loss: 1.0822\n",
      "Epoch: 4, Validation Loss: 1.1275\n",
      "Epoch: 5, Training Loss: 1.0850\n",
      "Epoch: 5, Validation Loss: 1.1195\n",
      "Epoch: 6, Training Loss: 1.0786\n",
      "Epoch: 6, Validation Loss: 1.0916\n",
      "Epoch: 7, Training Loss: 1.0829\n",
      "Epoch: 7, Validation Loss: 1.0885\n",
      "Epoch: 8, Training Loss: 1.0660\n",
      "Epoch: 8, Validation Loss: 1.2259\n",
      "Epoch: 9, Training Loss: 1.0667\n",
      "Epoch: 9, Validation Loss: 1.0647\n",
      "Epoch: 10, Training Loss: 1.0665\n",
      "Epoch: 10, Validation Loss: 1.0694\n",
      "Epoch: 11, Training Loss: 1.0632\n",
      "Epoch: 11, Validation Loss: 1.1220\n",
      "Epoch: 12, Training Loss: 1.0701\n",
      "Epoch: 12, Validation Loss: 1.0757\n",
      "Epoch: 13, Training Loss: 1.0686\n",
      "Epoch: 13, Validation Loss: 1.0695\n",
      "Epoch: 14, Training Loss: 1.0621\n",
      "Epoch: 14, Validation Loss: 1.0535\n",
      "Epoch: 15, Training Loss: 1.0656\n",
      "Epoch: 15, Validation Loss: 1.0649\n",
      "Epoch: 16, Training Loss: 1.0627\n",
      "Epoch: 16, Validation Loss: 1.0797\n",
      "Epoch: 17, Training Loss: 1.0631\n",
      "Epoch: 17, Validation Loss: 1.1010\n",
      "Epoch: 18, Training Loss: 1.0676\n",
      "Epoch: 18, Validation Loss: 1.0855\n",
      "Epoch: 19, Training Loss: 1.0697\n",
      "Epoch: 19, Validation Loss: 1.0881\n",
      "Epoch: 20, Training Loss: 1.0829\n",
      "Epoch: 20, Validation Loss: 1.0582\n",
      "Epoch: 21, Training Loss: 1.0706\n",
      "Epoch: 21, Validation Loss: 1.0733\n",
      "Epoch: 22, Training Loss: 1.0770\n",
      "Epoch: 22, Validation Loss: 1.0643\n",
      "Epoch: 23, Training Loss: 1.0796\n",
      "Epoch: 23, Validation Loss: 1.0560\n",
      "Epoch: 24, Training Loss: 1.0736\n",
      "Epoch: 24, Validation Loss: 1.1639\n",
      "Epoch: 25, Training Loss: 1.0719\n",
      "Epoch: 25, Validation Loss: 1.1343\n",
      "Epoch: 26, Training Loss: 1.0781\n",
      "Epoch: 26, Validation Loss: 1.0343\n",
      "Epoch: 27, Training Loss: 1.0999\n",
      "Epoch: 27, Validation Loss: 1.0318\n",
      "Epoch: 28, Training Loss: 1.0794\n",
      "Epoch: 28, Validation Loss: 1.0478\n",
      "Epoch: 29, Training Loss: 1.0774\n",
      "Epoch: 29, Validation Loss: 1.0880\n",
      "Epoch: 30, Training Loss: 1.0710\n",
      "Epoch: 30, Validation Loss: 1.0798\n",
      "Epoch: 31, Training Loss: 1.0759\n",
      "Epoch: 31, Validation Loss: 1.1603\n",
      "Epoch: 32, Training Loss: 1.0716\n",
      "Epoch: 32, Validation Loss: 1.0840\n",
      "Epoch: 33, Training Loss: 1.0889\n",
      "Epoch: 33, Validation Loss: 1.1171\n",
      "Epoch: 34, Training Loss: 1.1083\n",
      "Epoch: 34, Validation Loss: 1.0473\n",
      "Epoch: 35, Training Loss: 1.0688\n",
      "Epoch: 35, Validation Loss: 1.0824\n",
      "Epoch: 36, Training Loss: 1.0773\n",
      "Epoch: 36, Validation Loss: 1.0817\n",
      "Epoch: 37, Training Loss: 1.0701\n",
      "Epoch: 37, Validation Loss: 1.0338\n",
      "Epoch: 38, Training Loss: 1.0872\n",
      "Epoch: 38, Validation Loss: 1.0615\n",
      "Epoch: 39, Training Loss: 1.0878\n",
      "Epoch: 39, Validation Loss: 1.0620\n",
      "Epoch: 40, Training Loss: 1.0969\n",
      "Epoch: 40, Validation Loss: 1.1067\n",
      "Epoch: 41, Training Loss: 1.0876\n",
      "Epoch: 41, Validation Loss: 1.0816\n",
      "Epoch: 42, Training Loss: 1.0761\n",
      "Epoch: 42, Validation Loss: 1.0541\n",
      "Epoch: 43, Training Loss: 1.0659\n",
      "Epoch: 43, Validation Loss: 1.0402\n",
      "Epoch: 44, Training Loss: 1.0805\n",
      "Epoch: 44, Validation Loss: 1.0725\n",
      "Epoch: 45, Training Loss: 1.0603\n",
      "Epoch: 45, Validation Loss: 1.0506\n",
      "Epoch: 46, Training Loss: 1.0788\n",
      "Epoch: 46, Validation Loss: 1.0922\n",
      "Epoch: 47, Training Loss: 1.0683\n",
      "Epoch: 47, Validation Loss: 1.0403\n",
      "Epoch: 48, Training Loss: 1.0707\n",
      "Epoch: 48, Validation Loss: 1.0519\n",
      "Epoch: 49, Training Loss: 1.0707\n",
      "Epoch: 49, Validation Loss: 1.1332\n",
      "Epoch: 50, Training Loss: 1.0760\n",
      "Epoch: 50, Validation Loss: 1.0775\n",
      "Epoch: 51, Training Loss: 1.0655\n",
      "Epoch: 51, Validation Loss: 1.0268\n",
      "Epoch: 52, Training Loss: 1.0745\n",
      "Epoch: 52, Validation Loss: 1.0516\n",
      "Epoch: 53, Training Loss: 1.0683\n",
      "Epoch: 53, Validation Loss: 1.0579\n",
      "Epoch: 54, Training Loss: 1.0715\n",
      "Epoch: 54, Validation Loss: 1.0437\n",
      "Epoch: 55, Training Loss: 1.0979\n",
      "Epoch: 55, Validation Loss: 1.1118\n",
      "Epoch: 56, Training Loss: 1.1317\n",
      "Epoch: 56, Validation Loss: 1.1011\n",
      "Epoch: 57, Training Loss: 1.0734\n",
      "Epoch: 57, Validation Loss: 1.0494\n",
      "Epoch: 58, Training Loss: 1.0845\n",
      "Epoch: 58, Validation Loss: 1.1542\n",
      "Epoch: 59, Training Loss: 1.0818\n",
      "Epoch: 59, Validation Loss: 1.0213\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "318acd5b5f554267bc8f4ee94737afe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>val_loss</td><td>█▆▃▄▃▃▂▂▃▂▂▃▃▃▃▂▅▄▁▂▃▅▄▂▃▁▂▄▂▂▂▃▂▄▁▂▂▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>59</td></tr><tr><td>train_loss</td><td>1.08184</td></tr><tr><td>val_loss</td><td>1.02129</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-sweep-13</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/u71imis3\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/u71imis3</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_222250-u71imis3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y01drwu0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01024296558917539\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: add\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d07e265b7e40dfb636fbaf7a8f5ac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01669471666670385, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_231511-y01drwu0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/y01drwu0\" target=\"_blank\">bright-sweep-14</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/y01drwu0\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/y01drwu0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=add, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 1.4400\n",
      "Epoch: 0, Validation Loss: 1.2167\n",
      "Epoch: 1, Training Loss: 1.2006\n",
      "Epoch: 1, Validation Loss: 1.2177\n",
      "Epoch: 2, Training Loss: 1.1827\n",
      "Epoch: 2, Validation Loss: 1.1828\n",
      "Epoch: 3, Training Loss: 1.1558\n",
      "Epoch: 3, Validation Loss: 1.1632\n",
      "Epoch: 4, Training Loss: 1.0564\n",
      "Epoch: 4, Validation Loss: 1.0295\n",
      "Epoch: 5, Training Loss: 1.0371\n",
      "Epoch: 5, Validation Loss: 1.0424\n",
      "Epoch: 6, Training Loss: 1.0201\n",
      "Epoch: 6, Validation Loss: 1.0137\n",
      "Epoch: 7, Training Loss: 1.0144\n",
      "Epoch: 7, Validation Loss: 1.0074\n",
      "Epoch: 8, Training Loss: 1.0023\n",
      "Epoch: 8, Validation Loss: 1.0143\n",
      "Epoch: 9, Training Loss: 1.0006\n",
      "Epoch: 9, Validation Loss: 1.0030\n",
      "Epoch: 10, Training Loss: 0.9905\n",
      "Epoch: 10, Validation Loss: 0.9814\n",
      "Epoch: 11, Training Loss: 0.9788\n",
      "Epoch: 11, Validation Loss: 0.9706\n",
      "Epoch: 12, Training Loss: 0.9789\n",
      "Epoch: 12, Validation Loss: 0.9708\n",
      "Epoch: 13, Training Loss: 0.9805\n",
      "Epoch: 13, Validation Loss: 0.9489\n",
      "Epoch: 14, Training Loss: 0.9679\n",
      "Epoch: 14, Validation Loss: 0.9493\n",
      "Epoch: 15, Training Loss: 0.9638\n",
      "Epoch: 15, Validation Loss: 0.9462\n",
      "Epoch: 16, Training Loss: 0.9621\n",
      "Epoch: 16, Validation Loss: 0.9716\n",
      "Epoch: 17, Training Loss: 0.9558\n",
      "Epoch: 17, Validation Loss: 0.9500\n",
      "Epoch: 18, Training Loss: 0.9597\n",
      "Epoch: 18, Validation Loss: 0.9838\n",
      "Epoch: 19, Training Loss: 0.9484\n",
      "Epoch: 19, Validation Loss: 0.9456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59ab527367a486ba04bac580c463b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.044127…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▇▃▃▃▃▃▂▂▂▂▁▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_loss</td><td>0.94839</td></tr><tr><td>val_loss</td><td>0.94563</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-sweep-14</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/y01drwu0\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/y01drwu0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_231511-y01drwu0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a4rgf5m9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcriterion: MSELoss()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_1: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_2: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim_3: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tin_channels: 11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.06279269909246481\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnnconv_aggr: max\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tout_channels: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/liuxr/Desktop/gnn/wandb/run-20230522_232859-a4rgf5m9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/a4rgf5m9\" target=\"_blank\">ethereal-sweep-15</a></strong> to <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/sweeps/l48o87j4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/a4rgf5m9\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/a4rgf5m9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNNModel(\n",
      "  (conv1): NNConv(11, 64, aggr=max, nn=Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=704, bias=True)\n",
      "  ))\n",
      "  (global_pool): MaxAggregation()\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "Epoch: 0, Training Loss: 9.5224\n",
      "Epoch: 0, Validation Loss: 1.3043\n",
      "Epoch: 1, Training Loss: 1.3172\n",
      "Epoch: 1, Validation Loss: 1.3233\n",
      "Epoch: 2, Training Loss: 1.3068\n",
      "Epoch: 2, Validation Loss: 1.3416\n",
      "Epoch: 3, Training Loss: 1.3019\n",
      "Epoch: 3, Validation Loss: 1.2866\n",
      "Epoch: 4, Training Loss: 1.2981\n",
      "Epoch: 4, Validation Loss: 1.2893\n",
      "Epoch: 5, Training Loss: 1.2976\n",
      "Epoch: 5, Validation Loss: 1.2743\n",
      "Epoch: 6, Training Loss: 1.3129\n",
      "Epoch: 6, Validation Loss: 1.2763\n",
      "Epoch: 7, Training Loss: 1.2949\n",
      "Epoch: 7, Validation Loss: 1.2915\n",
      "Epoch: 8, Training Loss: 1.2960\n",
      "Epoch: 8, Validation Loss: 1.3340\n",
      "Epoch: 9, Training Loss: 1.2999\n",
      "Epoch: 9, Validation Loss: 1.3163\n",
      "Epoch: 10, Training Loss: 1.3060\n",
      "Epoch: 10, Validation Loss: 1.3024\n",
      "Epoch: 11, Training Loss: 1.3110\n",
      "Epoch: 11, Validation Loss: 1.2904\n",
      "Epoch: 12, Training Loss: 1.3047\n",
      "Epoch: 12, Validation Loss: 1.2970\n",
      "Epoch: 13, Training Loss: 1.3033\n",
      "Epoch: 13, Validation Loss: 1.3058\n",
      "Epoch: 14, Training Loss: 1.3084\n",
      "Epoch: 14, Validation Loss: 1.2862\n",
      "Epoch: 15, Training Loss: 1.3207\n",
      "Epoch: 15, Validation Loss: 1.3865\n",
      "Epoch: 16, Training Loss: 1.3106\n",
      "Epoch: 16, Validation Loss: 1.2771\n",
      "Epoch: 17, Training Loss: 1.3271\n",
      "Epoch: 17, Validation Loss: 1.3410\n",
      "Epoch: 18, Training Loss: 1.3377\n",
      "Epoch: 18, Validation Loss: 1.4149\n",
      "Epoch: 19, Training Loss: 1.3120\n",
      "Epoch: 19, Validation Loss: 1.2809\n",
      "Epoch: 20, Training Loss: 1.3040\n",
      "Epoch: 20, Validation Loss: 1.3086\n",
      "Epoch: 21, Training Loss: 1.3127\n",
      "Epoch: 21, Validation Loss: 1.3189\n",
      "Epoch: 22, Training Loss: 1.3115\n",
      "Epoch: 22, Validation Loss: 1.2941\n",
      "Epoch: 23, Training Loss: 1.3185\n",
      "Epoch: 23, Validation Loss: 1.3066\n",
      "Epoch: 24, Training Loss: 1.3177\n",
      "Epoch: 24, Validation Loss: 1.3392\n",
      "Epoch: 25, Training Loss: 1.3197\n",
      "Epoch: 25, Validation Loss: 1.2916\n",
      "Epoch: 26, Training Loss: 1.3225\n",
      "Epoch: 26, Validation Loss: 1.3341\n",
      "Epoch: 27, Training Loss: 1.3120\n",
      "Epoch: 27, Validation Loss: 1.2936\n",
      "Epoch: 28, Training Loss: 1.3195\n",
      "Epoch: 28, Validation Loss: 1.3478\n",
      "Epoch: 29, Training Loss: 1.3155\n",
      "Epoch: 29, Validation Loss: 1.4770\n",
      "Epoch: 30, Training Loss: 1.3105\n",
      "Epoch: 30, Validation Loss: 1.3525\n",
      "Epoch: 31, Training Loss: 1.3147\n",
      "Epoch: 31, Validation Loss: 1.4071\n",
      "Epoch: 32, Training Loss: 1.3114\n",
      "Epoch: 32, Validation Loss: 1.3164\n",
      "Epoch: 33, Training Loss: 1.3156\n",
      "Epoch: 33, Validation Loss: 1.3065\n",
      "Epoch: 34, Training Loss: 1.3180\n",
      "Epoch: 34, Validation Loss: 1.3067\n",
      "Epoch: 35, Training Loss: 1.3138\n",
      "Epoch: 35, Validation Loss: 1.2956\n",
      "Epoch: 36, Training Loss: 1.3173\n",
      "Epoch: 36, Validation Loss: 1.2906\n",
      "Epoch: 37, Training Loss: 1.3188\n",
      "Epoch: 37, Validation Loss: 1.3127\n",
      "Epoch: 38, Training Loss: 1.3135\n",
      "Epoch: 38, Validation Loss: 1.2969\n",
      "Epoch: 39, Training Loss: 1.3175\n",
      "Epoch: 39, Validation Loss: 1.2965\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b0b65eafc04e58a35fa4559d61c72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▃▃▁▂▁▁▂▃▂▂▂▂▂▁▅▁▃▆▁▂▃▂▂▃▂▃▂▄█▄▆▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_loss</td><td>1.31752</td></tr><tr><td>val_loss</td><td>1.29654</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-sweep-15</strong> at: <a href=\"https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/a4rgf5m9\" target=\"_blank\">https://wandb.ai/ntuwb/adam_mse_nnconv_poolMaxAggr_2fc_wandb_random_new/runs/a4rgf5m9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230522_232859-a4rgf5m9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np \n",
    "import random\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Define sweep config\n",
    "sweep_configuration = {\n",
    "'method': 'random',\n",
    "'metric': {'goal': 'minimize', 'name': 'val_loss'},\n",
    "'parameters': {'batch_size': {'distribution': 'q_log_uniform_values',\n",
    "                               'max': 256,\n",
    "                               'min': 16,\n",
    "                               'q': 8},\n",
    "                'criterion': {'value': 'MSELoss()'},\n",
    "                'epochs': {'values': [20,40,60]},\n",
    "                'hidden_dim_1': {'value': 64},\n",
    "                'hidden_dim_2': {'value': 128},\n",
    "                'hidden_dim_3': {'value': 32},\n",
    "                'in_channels': {'value': 11},\n",
    "                'lr': {'distribution': 'uniform',\n",
    "                                  'max': 0.1,\n",
    "                                  'min': 0},\n",
    "                'nnconv_aggr': {'values': ['max','mean','add']},\n",
    "                'optimizer': {'value': 'adam'},\n",
    "                'out_channels': {'value': 1}\n",
    "              }\n",
    "}\n",
    "\n",
    "# Initialize sweep by passing in config. \n",
    "# (Optional) Provide a name of the project.\n",
    "sweep_id = wandb.sweep(\n",
    "  sweep=sweep_configuration, \n",
    "  project=model_name\n",
    "  )\n",
    "\n",
    "# Define the dataset and do transformation\n",
    "class TargetTransform:\n",
    "    def __call__(self, data):\n",
    "        # Specify target.\n",
    "        target = 0 # the first property is the one to be predicted\n",
    "        data.y = data.y[:, target]\n",
    "        return data\n",
    "    \n",
    "def data(bs):\n",
    "    path = './datasets/QM9'\n",
    "    transform = T.Compose([TargetTransform(), T.Distance(norm=False)]) # add the distance into edge attributes\n",
    "    dataset = QM9(path, transform=transform)\n",
    "    # Split datasets.\n",
    "    torch.manual_seed(12345)\n",
    "    dataset = dataset.shuffle()\n",
    "    test_dataset = dataset[:10000]\n",
    "    val_dataset = dataset[10000:20000]\n",
    "    train_dataset = dataset[20000:]\n",
    "    test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False) #10000\n",
    "    val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False) #10000\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True) #110831\n",
    "    return test_loader, train_loader, val_loader\n",
    "\n",
    "# Define training function that takes in hyperparameter \n",
    "# values from `wandb.config` and uses them to train a \n",
    "# model and return metric\n",
    "def train_one_epoch(model, train_loader, epoch, lr, optimizer): \n",
    "    # Train step\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch).flatten()\n",
    "        loss = nn.MSELoss()\n",
    "        loss_output = loss(output, batch.y)\n",
    "        loss_output.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss_output * batch.num_graphs\n",
    "\n",
    "    average_loss = total_loss  / len(train_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Training Loss: {average_loss:.4f}')\n",
    "    logging.info(f'Epoch: {epoch}, Training Loss: {average_loss:.4f}')\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "def evaluate_one_epoch(model, val_loader, epoch, lr): \n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data).flatten()\n",
    "            val_loss = nn.MSELoss()\n",
    "            val_loss_output = val_loss(output, data.y)\n",
    "            total_val_loss += val_loss_output * data.num_graphs\n",
    "\n",
    "    average_val_loss = total_val_loss / len(val_loader.dataset)\n",
    "    \n",
    "    print(f'Epoch: {epoch}, Validation Loss: {average_val_loss:.4f}')\n",
    "    logging.info(f'Epoch: {epoch}, Validation Loss: {average_val_loss:.4f}')\n",
    "    \n",
    "    return average_val_loss\n",
    "\n",
    "# Define the GNN model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim_1, hidden_dim_2, hidden_dim_3, out_channels, nnconv_aggr):\n",
    "        super(GNNModel, self).__init__()\n",
    "        #nn – A neural network that maps edge features edge_attr of shape [-1, num_edge_features] to shape [-1, in_channels * out_channels], e.g., defined by torch.nn.Sequential.\n",
    "        nn1 = nn.Sequential(nn.Linear(5, 128), ReLU(), nn.Linear(128, in_channels * hidden_dim_1)) # maps to in_channels * out_channels\n",
    "        self.conv1 = NNConv(in_channels, hidden_dim_1, nn1, aggr=nnconv_aggr) #11*64\n",
    "        self.global_pool = aggr.MaxAggregation()\n",
    "        self.fc1 = nn.Linear(hidden_dim_1, hidden_dim_3)\n",
    "        self.fc2 = nn.Linear(hidden_dim_3, out_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        x = self.conv1(x, edge_index, edge_attr).relu()\n",
    "        #x = global_mean_pool(x, batch)  # size = [batch_size, hidden_channels]\n",
    "        x = self.global_pool(x,batch)\n",
    "        x = self.fc1(x).relu()\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "def build_model(in_channels, hidden_dim_1, hidden_dim_2, hidden_dim_3, out_channels, nnconv_aggr):\n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    model = GNNModel(in_channels, hidden_dim_1, hidden_dim_2, hidden_dim_3, out_channels, nnconv_aggr)\n",
    "    print(model)\n",
    "    logging.info(model)\n",
    "    return model\n",
    "\n",
    "def build_optimizer(model, lr):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    return optimizer\n",
    "    \n",
    "def main():\n",
    "    run = wandb.init()\n",
    "\n",
    "    # note that we define values from `wandb.config`  \n",
    "    # instead of defining hard values\n",
    "    lr  =  wandb.config.lr\n",
    "    bs = wandb.config.batch_size\n",
    "    epochs = wandb.config.epochs\n",
    "    in_channels = wandb.config.in_channels\n",
    "    hidden_dim_1 = wandb.config.hidden_dim_1\n",
    "    hidden_dim_2 = wandb.config.hidden_dim_2\n",
    "    hidden_dim_3 = wandb.config.hidden_dim_3\n",
    "    out_channels = wandb.config.out_channels\n",
    "    nnconv_aggr = wandb.config.nnconv_aggr\n",
    "    criterion = wandb.config.criterion\n",
    "    \n",
    "    logging.basicConfig(filename = model_name+'.log',\n",
    "                    level = logging.INFO,\n",
    "                    format = '%(asctime)s:%(levelname)s:%(message)s')\n",
    " \n",
    "    logging.info(sweep_configuration)\n",
    "    \n",
    "    # Prepare data\n",
    "    test_loader, train_loader, val_loader = data(bs)\n",
    "    \n",
    "    # Build gnn model\n",
    "    model = build_model(in_channels, hidden_dim_1, hidden_dim_2, hidden_dim_3, out_channels, nnconv_aggr)\n",
    "    \n",
    "    # Build optimizer\n",
    "    optimizer = build_optimizer(model, lr)\n",
    "    \n",
    "    # Start training and validation process\n",
    "    min_valid_loss = np.inf\n",
    "    loss_values = []\n",
    "    val_loss_values = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, epoch, lr, optimizer)\n",
    "        #loss_values.append(train_loss)\n",
    "        val_loss = evaluate_one_epoch(model, val_loader, epoch, lr)\n",
    "        #val_loss_values.append(val_loss)\n",
    "        wandb.log({\n",
    "            'epoch': epoch, \n",
    "            'train_loss': train_loss, \n",
    "            'val_loss': val_loss\n",
    "        })\n",
    "# Start sweep job.\n",
    "wandb.agent(sweep_id, function=main, count=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f12812",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./loss_values/loss_values_'+model_name+'.txt',loss_values)\n",
    "np.savetxt('./loss_values/val_loss_values_'+model_name+'.txt',val_loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c710901",
   "metadata": {},
   "source": [
    "By defining the message() and update() functions, you control how information is propagated through the graph and how nodes update their representations based on the received messages. These functions enable the GNN to learn from the graph structure and capture useful information for the prediction task.\n",
    "\n",
    "During the forward() pass, the GNN iteratively performs message passing and aggregation steps across the graph, combining information from neighboring nodes to update each node's representation. The output of the forward() function provides the final representations that can be further processed or used for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b1599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test after training\n",
    "model.eval()\n",
    "total_test_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data).flatten()\n",
    "        test_loss = config[\"criterion\"](output, data.y)\n",
    "        total_test_loss += test_loss.item() * data.num_graphs\n",
    "\n",
    "average_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "print(f'Test Loss before training: {average_test_loss_before:.4f}')\n",
    "print(f'Test Loss after training: {average_test_loss:.4f}')\n",
    "logging.info(f'Test Loss before training: {average_test_loss_before:.4f}')\n",
    "logging.info(f'Test Loss after training: {average_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = np.loadtxt(\"./loss_values/loss_values_\"+model_name+\".txt\")\n",
    "valid_loss_values = np.loadtxt(\"./loss_values/val_loss_values_\"+model_name+\".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a50284",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_values, label='train loss:'+model_name, ls='-', ms=20, markevery=100, alpha=0.5)\n",
    "plt.plot(valid_loss_values,'y', label='validation loss:'+model_name, ls='-', ms=20, markevery=100, alpha=0.8)\n",
    "\n",
    "#don't know why the markevery doesn't work\n",
    "plt.xlabel('epoch') \n",
    "plt.ylabel('loss') \n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cb9b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(11, 64)\n",
      "  (conv2): GCNConv(64, 64)\n",
      "  (conv3): GCNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
